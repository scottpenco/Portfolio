{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in story as sample txt into python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 2649881\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MUSASHI \n",
      "\n",
      "\n",
      "By Eiji Yoshikawa \n",
      "\n",
      "Translated from the Japanese by Charles S. Terry \n",
      "Foreword by Edwin O. Reischauer \n",
      "\n",
      "\n",
      "\n",
      "Kodansha Internatio\n"
     ]
    }
   ],
   "source": [
    "with open(\"/Users/Scott/Library/Mobile Documents/com~apple~CloudDocs/CVs and LOMs/Portfolio/DS 2024/GPT architeture/Musashi.txt\", 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[0:149])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of tokens in the text are: 596013\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print('The amount of tokens in the text are:' , len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Tokens into Token Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20044\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token: integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '\"': 1,\n",
       " \"'\": 2,\n",
       " '(': 3,\n",
       " ')': 4,\n",
       " '*': 5,\n",
       " ',': 6,\n",
       " '-eight': 7,\n",
       " '-five': 8,\n",
       " '-seven': 9,\n",
       " '-six': 10,\n",
       " '.': 11,\n",
       " '/': 12,\n",
       " '/His': 13,\n",
       " '/Run': 14,\n",
       " '/clopping': 15,\n",
       " '/more': 16,\n",
       " '0': 17,\n",
       " '0-over': 18,\n",
       " '000': 19,\n",
       " '05': 20,\n",
       " '06': 21,\n",
       " '07': 22,\n",
       " '08': 23,\n",
       " '09': 24,\n",
       " '1': 25,\n",
       " '1-chome': 26,\n",
       " '10': 27,\n",
       " '100': 28,\n",
       " '11': 29,\n",
       " '1128652': 30,\n",
       " '12': 31,\n",
       " '120': 32,\n",
       " '130': 33,\n",
       " '1333': 34,\n",
       " '1352': 35,\n",
       " '1460s': 36,\n",
       " '15': 37,\n",
       " '1544': 38,\n",
       " '1550s': 39,\n",
       " '1560s': 40,\n",
       " '1582': 41,\n",
       " '1584': 42,\n",
       " '1590': 43,\n",
       " '1598': 44,\n",
       " '16': 45,\n",
       " '1600': 46,\n",
       " '1601': 47,\n",
       " '1605': 48,\n",
       " '1609': 49,\n",
       " '1612': 50,\n",
       " '1612]': 51,\n",
       " '1614': 52,\n",
       " '1615': 53,\n",
       " '1637-38': 54,\n",
       " '1640': 55,\n",
       " '1645': 56,\n",
       " '17': 57,\n",
       " '17-14': 58,\n",
       " '18': 59,\n",
       " '1868': 60,\n",
       " '1892-1962': 61,\n",
       " '19': 62,\n",
       " '190': 63,\n",
       " '1910': 64,\n",
       " '1929': 65,\n",
       " '1935': 66,\n",
       " '1939': 67,\n",
       " '1961': 68,\n",
       " '1966': 69,\n",
       " '1971': 70,\n",
       " '1981': 71,\n",
       " '1990': 72,\n",
       " '1995': 73,\n",
       " '20': 74,\n",
       " '208-span': 75,\n",
       " '21': 76,\n",
       " '22': 77,\n",
       " '23': 78,\n",
       " '24': 79,\n",
       " '25': 80,\n",
       " '265': 81,\n",
       " '28': 82,\n",
       " '33': 83,\n",
       " '4-7700-1957-2': 84,\n",
       " '477': 85,\n",
       " '500': 86,\n",
       " '53-volume': 87,\n",
       " '6': 88,\n",
       " '70s': 89,\n",
       " '80-8791': 90,\n",
       " '978-4-7700-1957-8': 91,\n",
       " ':': 92,\n",
       " ';': 93,\n",
       " '?': 94,\n",
       " 'A': 95,\n",
       " 'A-o-o-oh': 96,\n",
       " 'A-w-r-g': 97,\n",
       " 'A-w-w-ful': 98,\n",
       " 'A-w-w-k': 99,\n",
       " 'A11': 100,\n",
       " 'AND': 101,\n",
       " 'Aaii': 102,\n",
       " 'Aarrgghh': 103,\n",
       " 'Abandoning': 104,\n",
       " 'Abashed': 105,\n",
       " 'Abbot': 106,\n",
       " 'Abduction': 107,\n",
       " 'Aboard': 108,\n",
       " 'About': 109,\n",
       " 'Above': 110,\n",
       " 'Abruptly': 111,\n",
       " 'Absolutely': 112,\n",
       " 'Absorbed': 113,\n",
       " 'Abura': 114,\n",
       " 'Academy': 115,\n",
       " 'Accepting': 116,\n",
       " 'Accidents': 117,\n",
       " 'According': 118,\n",
       " 'Accustomed': 119,\n",
       " 'Acknowledge': 120,\n",
       " 'Acock': 121,\n",
       " 'Acolyte': 122,\n",
       " 'Across': 123,\n",
       " 'Acting': 124,\n",
       " 'Actually': 125,\n",
       " 'Adams': 126,\n",
       " 'Added': 127,\n",
       " 'Addressing': 128,\n",
       " 'Adenoids': 129,\n",
       " 'Admit': 130,\n",
       " 'Admitting': 131,\n",
       " 'Advancing': 132,\n",
       " 'Advising': 133,\n",
       " 'Affair': 134,\n",
       " 'Afraid': 135,\n",
       " 'After': 136,\n",
       " 'Afterward': 137,\n",
       " 'Again': 138,\n",
       " 'Against': 139,\n",
       " 'Age': 140,\n",
       " 'Aged': 141,\n",
       " 'Aggravating': 142,\n",
       " 'Agon': 143,\n",
       " 'Ah': 144,\n",
       " 'Ah-h-h': 145,\n",
       " 'Ahead': 146,\n",
       " 'Aida': 147,\n",
       " 'Aita': 148,\n",
       " 'Ajar': 149,\n",
       " 'Akagi': 150,\n",
       " 'Akakabe': 151,\n",
       " 'Akamatsu': 152,\n",
       " 'Akamatsus': 153,\n",
       " 'Akashi': 154,\n",
       " 'Akechi': 155,\n",
       " 'Akemi': 156,\n",
       " 'Akemi—that': 157,\n",
       " 'Akemi—the': 158,\n",
       " 'Alas': 159,\n",
       " 'Alert': 160,\n",
       " 'Alfred': 161,\n",
       " 'Alight': 162,\n",
       " 'Alive': 163,\n",
       " 'All': 164,\n",
       " 'Almost': 165,\n",
       " 'Aloeswood': 166,\n",
       " 'Along': 167,\n",
       " 'Aloud': 168,\n",
       " 'Already': 169,\n",
       " 'Also': 170,\n",
       " 'Although': 171,\n",
       " 'Altogether': 172,\n",
       " 'Always': 173,\n",
       " 'Am': 174,\n",
       " 'Amako': 175,\n",
       " 'Amami': 176,\n",
       " 'Amano': 177,\n",
       " 'Amaterasu': 178,\n",
       " 'Amateur': 179,\n",
       " 'Amayumi': 180,\n",
       " 'Amazed': 181,\n",
       " 'Amazing': 182,\n",
       " 'Ambassador': 183,\n",
       " 'America': 184,\n",
       " 'Americans': 185,\n",
       " 'Amid': 186,\n",
       " 'Amida': 187,\n",
       " 'Amigasa': 188,\n",
       " 'Amigasa-jaya': 189,\n",
       " 'Ammunition': 190,\n",
       " 'Amoi': 191,\n",
       " 'Among': 192,\n",
       " 'Amusement': 193,\n",
       " 'An': 194,\n",
       " 'Analects': 195,\n",
       " 'Ananda': 196,\n",
       " 'Ancient': 197,\n",
       " 'And': 198,\n",
       " 'Anger': 199,\n",
       " 'Angered': 200,\n",
       " 'Angrily': 201,\n",
       " 'Angry': 202,\n",
       " 'Anguish': 203,\n",
       " 'Animals': 204,\n",
       " 'Announcement': 205,\n",
       " 'Annoyed': 206,\n",
       " 'Ano': 207,\n",
       " 'Another': 208,\n",
       " 'Answer': 209,\n",
       " 'Answering': 210,\n",
       " 'Answer—In': 211,\n",
       " 'Anthology': 212,\n",
       " 'Ants': 213,\n",
       " 'Any': 214,\n",
       " 'Anybody': 215,\n",
       " 'Anyhow': 216,\n",
       " 'Anyone': 217,\n",
       " 'Anything': 218,\n",
       " 'Anyway': 219,\n",
       " 'Anywhere': 220,\n",
       " 'Aoba': 221,\n",
       " 'Aoki': 222,\n",
       " 'Aoyama': 223,\n",
       " 'Apart': 224,\n",
       " 'Apian': 225,\n",
       " 'Apologize': 226,\n",
       " 'Apology': 227,\n",
       " 'Appalled': 228,\n",
       " 'Apparently': 229,\n",
       " 'Applied': 230,\n",
       " 'Approaching': 231,\n",
       " 'Ara': 232,\n",
       " 'Araki': 233,\n",
       " 'Arakida': 234,\n",
       " 'Arakidas': 235,\n",
       " 'Ardent': 236,\n",
       " 'Are': 237,\n",
       " 'Areal': 238,\n",
       " 'Aren': 239,\n",
       " 'Arima': 240,\n",
       " 'Arisugawa': 241,\n",
       " 'Arming': 242,\n",
       " 'Army': 243,\n",
       " 'Around': 244,\n",
       " 'Arrest': 245,\n",
       " 'Arrgh': 246,\n",
       " 'Arriving': 247,\n",
       " 'Arrowheads': 248,\n",
       " 'Art': 249,\n",
       " 'Artisans': 250,\n",
       " 'As': 251,\n",
       " 'Asagaya': 252,\n",
       " 'Asahi': 253,\n",
       " 'Asakusa': 254,\n",
       " 'Asama': 255,\n",
       " 'Asano': 256,\n",
       " 'Asaya': 257,\n",
       " 'Ashes': 258,\n",
       " 'Ashikaga': 259,\n",
       " 'Ashikagas': 260,\n",
       " 'Aside': 261,\n",
       " 'Ask': 262,\n",
       " 'Asked': 263,\n",
       " 'Asking': 264,\n",
       " 'Assembling': 265,\n",
       " 'Assistant': 266,\n",
       " 'Association': 267,\n",
       " 'Assured': 268,\n",
       " 'Astonished': 269,\n",
       " 'Astounded': 270,\n",
       " 'At': 271,\n",
       " 'Atop': 272,\n",
       " 'Atsugi': 273,\n",
       " 'Atsuta': 274,\n",
       " 'Attack': 275,\n",
       " 'Attempting': 276,\n",
       " 'Attendant': 277,\n",
       " 'Attendants': 278,\n",
       " 'Attuned': 279,\n",
       " 'Auntie': 280,\n",
       " 'Autumn': 281,\n",
       " 'Avenue': 282,\n",
       " 'Avery': 283,\n",
       " 'Aw': 284,\n",
       " 'Awa': 285,\n",
       " 'Awaji': 286,\n",
       " 'Awataguchi': 287,\n",
       " 'Ayabe': 288,\n",
       " 'Ayamachi': 289,\n",
       " 'Azabu': 290,\n",
       " 'BOOK': 291,\n",
       " 'Ba-ba-bakurocho': 292,\n",
       " 'Babies': 293,\n",
       " 'Back': 294,\n",
       " 'Bad': 295,\n",
       " 'Baiken': 296,\n",
       " 'Bakurocho': 297,\n",
       " 'Balling': 298,\n",
       " 'Ban': 299,\n",
       " 'Bandits': 300,\n",
       " 'Banryti': 301,\n",
       " 'Banryu': 302,\n",
       " 'Barely': 303,\n",
       " 'Bark': 304,\n",
       " 'Barkers': 305,\n",
       " 'Barley': 306,\n",
       " 'Basket': 307,\n",
       " 'Bastard': 308,\n",
       " 'Bastards': 309,\n",
       " 'Bathed': 310,\n",
       " 'Battle': 311,\n",
       " 'Bay': 312,\n",
       " 'Be': 313,\n",
       " 'Beads': 314,\n",
       " 'Beaming': 315,\n",
       " 'Bear': 316,\n",
       " 'Beard': 317,\n",
       " 'Beast': 318,\n",
       " 'Beasts': 319,\n",
       " 'Beauty': 320,\n",
       " 'Because': 321,\n",
       " 'Beckoned': 322,\n",
       " 'Beckoning': 323,\n",
       " 'Become': 324,\n",
       " 'Before': 325,\n",
       " 'Began': 326,\n",
       " 'Begging': 327,\n",
       " 'Beginners': 328,\n",
       " 'Beginning': 329,\n",
       " 'Behind': 330,\n",
       " 'Being': 331,\n",
       " 'Believe': 332,\n",
       " 'Believing': 333,\n",
       " 'Bell': 334,\n",
       " 'Below': 335,\n",
       " 'Bending': 336,\n",
       " 'Beneath': 337,\n",
       " 'Benzo': 338,\n",
       " 'Beside': 339,\n",
       " 'Besides': 340,\n",
       " 'Better': 341,\n",
       " 'Between': 342,\n",
       " 'Bewildered': 343,\n",
       " 'Beyond': 344,\n",
       " 'Bidding': 345,\n",
       " 'Bide': 346,\n",
       " 'Big': 347,\n",
       " 'Birth': 348,\n",
       " 'Bitch': 349,\n",
       " 'Bite': 350,\n",
       " 'Biting': 351,\n",
       " 'Bits': 352,\n",
       " 'Bitter': 353,\n",
       " 'Biwa': 354,\n",
       " 'Bizen': 355,\n",
       " 'Bizoji': 356,\n",
       " 'Black': 357,\n",
       " 'Blackthorne': 358,\n",
       " 'Blessed': 359,\n",
       " 'Blinded': 360,\n",
       " 'Blindly': 361,\n",
       " 'Blinds': 362,\n",
       " 'Block': 363,\n",
       " 'Blocking': 364,\n",
       " 'Blood': 365,\n",
       " 'Bloodstains': 366,\n",
       " 'Blossom': 367,\n",
       " 'Blossoms': 368,\n",
       " 'Blow': 369,\n",
       " 'Blue': 370,\n",
       " 'Bluntly': 371,\n",
       " 'Blushing': 372,\n",
       " 'Boat': 373,\n",
       " 'Boats': 374,\n",
       " 'Bodhisattva': 375,\n",
       " 'Body': 376,\n",
       " 'Boiling': 377,\n",
       " 'Bokuden': 378,\n",
       " 'Bolstering': 379,\n",
       " 'Bon': 380,\n",
       " 'Bong': 381,\n",
       " 'Boo': 382,\n",
       " 'Book': 383,\n",
       " 'Books': 384,\n",
       " 'Bored': 385,\n",
       " 'Born': 386,\n",
       " 'Borrow': 387,\n",
       " 'Borrowing': 388,\n",
       " 'Bosatsu': 389,\n",
       " 'Both': 390,\n",
       " 'Bow': 391,\n",
       " 'Bowing': 392,\n",
       " 'Boy': 393,\n",
       " 'Boys': 394,\n",
       " 'Bracing': 395,\n",
       " 'Branches': 396,\n",
       " 'Brat': 397,\n",
       " 'Braver': 398,\n",
       " 'Breaking': 399,\n",
       " 'Breath': 400,\n",
       " 'Breathing': 401,\n",
       " 'Breathless': 402,\n",
       " 'Breeze': 403,\n",
       " 'Bridge': 404,\n",
       " 'Bridge—after': 405,\n",
       " 'Briefly': 406,\n",
       " 'Bright': 407,\n",
       " 'Bring': 408,\n",
       " 'Bringing': 409,\n",
       " 'Broken': 410,\n",
       " 'Brooding': 411,\n",
       " 'Brother': 412,\n",
       " 'Brushing': 413,\n",
       " 'Brute': 414,\n",
       " 'Buckwheat': 415,\n",
       " 'Buddha': 416,\n",
       " 'Buddhas': 417,\n",
       " 'Buddhism': 418,\n",
       " 'Buddhist': 419,\n",
       " 'Buddhists': 420,\n",
       " 'Building': 421,\n",
       " 'Buna': 422,\n",
       " 'Bungoro': 423,\n",
       " 'Bunkyo-ku': 424,\n",
       " 'Burning': 425,\n",
       " 'Burns': 426,\n",
       " 'Bursting': 427,\n",
       " 'Bury': 428,\n",
       " 'Business': 429,\n",
       " 'But': 430,\n",
       " 'Butterfly': 431,\n",
       " 'Butting': 432,\n",
       " 'But—': 433,\n",
       " 'Buy': 434,\n",
       " 'Buzen': 435,\n",
       " 'Buzzing': 436,\n",
       " 'By': 437,\n",
       " 'Bye': 438,\n",
       " 'Bynner': 439,\n",
       " 'C-c-coward': 440,\n",
       " 'CERTIFICATE': 441,\n",
       " 'Call': 442,\n",
       " 'Called': 443,\n",
       " 'Calling': 444,\n",
       " 'Calm': 445,\n",
       " 'Calmly': 446,\n",
       " 'Can': 447,\n",
       " 'Canceled': 448,\n",
       " 'Captain': 449,\n",
       " 'Capture': 450,\n",
       " 'Careful': 451,\n",
       " 'Carefully': 452,\n",
       " 'Cargo': 453,\n",
       " 'Carp': 454,\n",
       " 'Carried': 455,\n",
       " 'Carry': 456,\n",
       " 'Carrying': 457,\n",
       " 'Casting': 458,\n",
       " 'Castle': 459,\n",
       " 'Casual': 460,\n",
       " 'Catch': 461,\n",
       " 'Catching': 462,\n",
       " 'Caught': 463,\n",
       " 'Cause': 464,\n",
       " 'Caution': 465,\n",
       " 'Cawing': 466,\n",
       " 'Centuries': 467,\n",
       " 'Certainly': 468,\n",
       " 'Ch': 469,\n",
       " 'Challenge': 470,\n",
       " 'Chameleon-like': 471,\n",
       " 'Chang': 472,\n",
       " 'Change': 473,\n",
       " 'Changes': 474,\n",
       " 'Changing': 475,\n",
       " 'Chant': 476,\n",
       " 'Charge': 477,\n",
       " 'Charles': 478,\n",
       " 'Chase': 479,\n",
       " 'Chasing': 480,\n",
       " 'Chat': 481,\n",
       " 'Chatterbox': 482,\n",
       " 'Chattering': 483,\n",
       " 'Chatting': 484,\n",
       " 'Chawan': 485,\n",
       " 'Chaya': 486,\n",
       " 'Cheer': 487,\n",
       " 'Cherry': 488,\n",
       " 'Chestnuts': 489,\n",
       " 'Chewing': 490,\n",
       " 'Chichibu': 491,\n",
       " 'Chichibu—': 492,\n",
       " 'Chief': 493,\n",
       " 'Chii-i': 494,\n",
       " 'Children': 495,\n",
       " 'Chilled': 496,\n",
       " 'China': 497,\n",
       " 'Chinese': 498,\n",
       " 'Chinese-quince': 499,\n",
       " 'Chinese-white': 500,\n",
       " 'Chips': 501,\n",
       " 'Chirimazuka': 502,\n",
       " 'Chiyoda': 503,\n",
       " 'Choose': 504,\n",
       " 'Choosing': 505,\n",
       " 'Chosokabe': 506,\n",
       " 'Christian': 507,\n",
       " 'Chuang-tsu': 508,\n",
       " 'Chujo': 509,\n",
       " 'Circle': 510,\n",
       " 'Circumstantial': 511,\n",
       " 'City': 512,\n",
       " 'Civil': 513,\n",
       " 'Civilization': 514,\n",
       " 'Clad': 515,\n",
       " 'Clang': 516,\n",
       " 'Clapping': 517,\n",
       " 'Clasping': 518,\n",
       " 'Clavell': 519,\n",
       " 'Clean': 520,\n",
       " 'Cleansing': 521,\n",
       " 'Clear': 522,\n",
       " 'Clenching': 523,\n",
       " 'Clerks': 524,\n",
       " 'Clicking': 525,\n",
       " 'Climb': 526,\n",
       " 'Climbing': 527,\n",
       " 'Clinging': 528,\n",
       " 'Close': 529,\n",
       " 'Closer': 530,\n",
       " 'Closing': 531,\n",
       " 'Clothed': 532,\n",
       " 'Clouds': 533,\n",
       " 'Clutching': 534,\n",
       " 'Cocking': 535,\n",
       " 'Cold': 536,\n",
       " 'Cold-bloodedness': 537,\n",
       " 'Collect': 538,\n",
       " 'Comb': 539,\n",
       " 'Come': 540,\n",
       " 'Comfortable': 541,\n",
       " 'Coming': 542,\n",
       " 'Commissioner': 543,\n",
       " 'Compared': 544,\n",
       " 'Comparing': 545,\n",
       " 'Comparisons': 546,\n",
       " 'Compassion': 547,\n",
       " 'Completely': 548,\n",
       " 'Complicating': 549,\n",
       " 'Con': 550,\n",
       " 'Conciliation': 551,\n",
       " 'Condescension': 552,\n",
       " 'Confess': 553,\n",
       " 'Confessing': 554,\n",
       " 'Confidence': 555,\n",
       " 'Confronted': 556,\n",
       " 'Confronting': 557,\n",
       " 'Confucian': 558,\n",
       " 'Confucianism': 559,\n",
       " 'Confucius': 560,\n",
       " 'Congratulations': 561,\n",
       " 'Conscious': 562,\n",
       " 'Consequently': 563,\n",
       " 'Conservatively': 564,\n",
       " 'Consider': 565,\n",
       " 'Considerations': 566,\n",
       " 'Considering': 567,\n",
       " 'Conspicuous': 568,\n",
       " 'Construction': 569,\n",
       " 'Contemplating': 570,\n",
       " 'Contents': 571,\n",
       " 'Contrary': 572,\n",
       " 'Controlling': 573,\n",
       " 'Conversation': 574,\n",
       " 'Convinced': 575,\n",
       " 'Copying': 576,\n",
       " 'Copyright': 577,\n",
       " 'Cough': 578,\n",
       " 'Could': 579,\n",
       " 'Couldn': 580,\n",
       " 'Council': 581,\n",
       " 'Councilor': 582,\n",
       " 'Counting': 583,\n",
       " 'Court': 584,\n",
       " 'Cover': 585,\n",
       " 'Covering': 586,\n",
       " 'Coward': 587,\n",
       " 'Cowardice': 588,\n",
       " 'Cowards': 589,\n",
       " 'Cowed': 590,\n",
       " 'Cradling': 591,\n",
       " 'Craftsman': 592,\n",
       " 'Crag': 593,\n",
       " 'Crane': 594,\n",
       " 'Cravenness': 595,\n",
       " 'Crazy': 596,\n",
       " 'Created': 597,\n",
       " 'Cricket': 598,\n",
       " 'Crippled': 599,\n",
       " 'Crook': 600,\n",
       " 'Crooks': 601,\n",
       " 'Crossing': 602,\n",
       " 'Crossings': 603,\n",
       " 'Crouching': 604,\n",
       " 'Crows': 605,\n",
       " 'Cry': 606,\n",
       " 'Crying': 607,\n",
       " 'Cryptomeria': 608,\n",
       " 'Cupping': 609,\n",
       " 'Curious': 610,\n",
       " 'Curiously': 611,\n",
       " 'Curling': 612,\n",
       " 'Cursing': 613,\n",
       " 'Cut': 614,\n",
       " 'Daddy': 615,\n",
       " 'Daigo': 616,\n",
       " 'Daimon': 617,\n",
       " 'Daimonji': 618,\n",
       " 'Daimyojin': 619,\n",
       " 'Dainichi': 620,\n",
       " 'Dairinoura': 621,\n",
       " 'Daisenji': 622,\n",
       " 'Daishi': 623,\n",
       " 'Daishoji': 624,\n",
       " 'Daisuke': 625,\n",
       " 'Daito': 626,\n",
       " 'Daitokuji': 627,\n",
       " 'Daizo': 628,\n",
       " 'Daizos': 629,\n",
       " 'Daizo—he': 630,\n",
       " 'Damned': 631,\n",
       " 'Dampachi': 632,\n",
       " 'Dan': 633,\n",
       " 'Dance': 634,\n",
       " 'Danger': 635,\n",
       " 'Dangerous': 636,\n",
       " 'Dark': 637,\n",
       " 'Darkness': 638,\n",
       " 'Dashing': 639,\n",
       " 'Date': 640,\n",
       " 'Date—his': 641,\n",
       " 'Daun': 642,\n",
       " 'Dawn': 643,\n",
       " 'Day': 644,\n",
       " 'Daybreak': 645,\n",
       " 'Daylight': 646,\n",
       " 'Daytime': 647,\n",
       " 'Dayu': 648,\n",
       " 'Dazed': 649,\n",
       " 'Dead': 650,\n",
       " 'Death': 651,\n",
       " 'Debuchi': 652,\n",
       " 'Decide': 653,\n",
       " 'Deciding': 654,\n",
       " 'Deep': 655,\n",
       " 'Deeply': 656,\n",
       " 'Defeated': 657,\n",
       " 'Definitely': 658,\n",
       " 'Deflated': 659,\n",
       " 'Deliberately': 660,\n",
       " 'Delighted': 661,\n",
       " 'Demesne': 662,\n",
       " 'Demon': 663,\n",
       " 'Demon’s': 664,\n",
       " 'Denshichiro': 665,\n",
       " 'Denshin': 666,\n",
       " 'Departed': 667,\n",
       " 'Depend': 668,\n",
       " 'Depends': 669,\n",
       " 'Deploring': 670,\n",
       " 'Descending': 671,\n",
       " 'Descriptions': 672,\n",
       " 'Deserted': 673,\n",
       " 'Desires': 674,\n",
       " 'Desperate': 675,\n",
       " 'Desperation': 676,\n",
       " 'Despite': 677,\n",
       " 'Destiny': 678,\n",
       " 'Determined': 679,\n",
       " 'Devilish': 680,\n",
       " 'Devils': 681,\n",
       " 'Dew': 682,\n",
       " 'Diagonally': 683,\n",
       " 'Diamond': 684,\n",
       " 'Did': 685,\n",
       " 'Didn': 686,\n",
       " 'Die': 687,\n",
       " 'Differences': 688,\n",
       " 'Difficult': 689,\n",
       " 'Digger': 690,\n",
       " 'Digging': 691,\n",
       " 'Dinner': 692,\n",
       " 'Directly': 693,\n",
       " 'Dirt': 694,\n",
       " 'Disappointed': 695,\n",
       " 'Disciple': 696,\n",
       " 'Disciples': 697,\n",
       " 'Disciplined': 698,\n",
       " 'Disconcerted': 699,\n",
       " 'Discovering': 700,\n",
       " 'Discuss': 701,\n",
       " 'Disguised': 702,\n",
       " 'Disgusted': 703,\n",
       " 'Dislodged': 704,\n",
       " 'Dismount': 705,\n",
       " 'Dismounting': 706,\n",
       " 'Dispensing': 707,\n",
       " 'Displaying': 708,\n",
       " 'Disregarding': 709,\n",
       " 'Disruptive': 710,\n",
       " 'Distributed': 711,\n",
       " 'Do': 712,\n",
       " 'Dodging': 713,\n",
       " 'Does': 714,\n",
       " 'Doesn': 715,\n",
       " 'Dogen': 716,\n",
       " 'Doing': 717,\n",
       " 'Dokan': 718,\n",
       " 'Don': 719,\n",
       " 'Done': 720,\n",
       " 'Donjiki': 721,\n",
       " 'Donjiki—well': 722,\n",
       " 'Donning': 723,\n",
       " 'Donor': 724,\n",
       " 'Doubt': 725,\n",
       " 'Douse': 726,\n",
       " 'Dowager': 727,\n",
       " 'Down': 728,\n",
       " 'Drag': 729,\n",
       " 'Dragged': 730,\n",
       " 'Dragging': 731,\n",
       " 'Drainage': 732,\n",
       " 'Drawing': 733,\n",
       " 'Drawn': 734,\n",
       " 'Dreading': 735,\n",
       " 'Dream': 736,\n",
       " 'Dreams': 737,\n",
       " 'Dressed': 738,\n",
       " 'Dried': 739,\n",
       " 'Drink': 740,\n",
       " 'Driven': 741,\n",
       " 'Drop': 742,\n",
       " 'Dropping': 743,\n",
       " 'Drowned': 744,\n",
       " 'Drowning': 745,\n",
       " 'Drumsticks': 746,\n",
       " 'Drying': 747,\n",
       " 'Ducking': 748,\n",
       " 'Due': 749,\n",
       " 'Dumb': 750,\n",
       " 'Dumbfounded': 751,\n",
       " 'During': 752,\n",
       " 'Dutifully': 753,\n",
       " 'EARTH': 754,\n",
       " 'Each': 755,\n",
       " 'Eager': 756,\n",
       " 'Eagerly': 757,\n",
       " 'Eagle': 758,\n",
       " 'Earlier': 759,\n",
       " 'Early': 760,\n",
       " 'Ears': 761,\n",
       " 'East': 762,\n",
       " 'Eastern': 763,\n",
       " 'Eat': 764,\n",
       " 'Ech': 765,\n",
       " 'Echigo': 766,\n",
       " 'Echizen': 767,\n",
       " 'Edging': 768,\n",
       " 'Edification': 769,\n",
       " 'Edo': 770,\n",
       " 'Edo—the': 771,\n",
       " 'Edwin': 772,\n",
       " 'Eh': 773,\n",
       " 'Ei': 774,\n",
       " 'Eight': 775,\n",
       " 'Eiji': 776,\n",
       " 'Either': 777,\n",
       " 'Elated': 778,\n",
       " 'Elders': 779,\n",
       " 'Elegant': 780,\n",
       " 'Eleven': 781,\n",
       " 'Elutes': 782,\n",
       " 'Embarrassed': 783,\n",
       " 'Emerging': 784,\n",
       " 'Emeritus': 785,\n",
       " 'Emperor': 786,\n",
       " 'Empress': 787,\n",
       " 'Empty': 788,\n",
       " 'Encircled': 789,\n",
       " 'Encounter': 790,\n",
       " 'Encouraged': 791,\n",
       " 'Ending': 792,\n",
       " 'Enemies': 793,\n",
       " 'English': 794,\n",
       " 'Enjoy': 795,\n",
       " 'Enlightenment': 796,\n",
       " 'Ennenji': 797,\n",
       " 'Enough': 798,\n",
       " 'Enraged': 799,\n",
       " 'Enryakuji': 800,\n",
       " 'Enshu': 801,\n",
       " 'Entering': 802,\n",
       " 'Entranced': 803,\n",
       " 'Entwining': 804,\n",
       " 'Enveloped': 805,\n",
       " 'Eook': 806,\n",
       " 'Er': 807,\n",
       " 'Escaped': 808,\n",
       " 'Especially': 809,\n",
       " 'Eternal': 810,\n",
       " 'Etiquette': 811,\n",
       " 'Europe': 812,\n",
       " 'Europeans': 813,\n",
       " 'Evading': 814,\n",
       " 'Eve': 815,\n",
       " 'Even': 816,\n",
       " 'Evening': 817,\n",
       " 'Evenings': 818,\n",
       " 'Eventually': 819,\n",
       " 'Ever': 820,\n",
       " 'Every': 821,\n",
       " 'Everybody': 822,\n",
       " 'Everyone': 823,\n",
       " 'Everything': 824,\n",
       " 'Everywhere': 825,\n",
       " 'Evidence': 826,\n",
       " 'Evidently': 827,\n",
       " 'Evil': 828,\n",
       " 'Exactly': 829,\n",
       " 'Examine': 830,\n",
       " 'Except': 831,\n",
       " 'Excited': 832,\n",
       " 'Exhausted': 833,\n",
       " 'Experienced': 834,\n",
       " 'Explain': 835,\n",
       " 'Expressing': 836,\n",
       " 'Extending': 837,\n",
       " 'Externally': 838,\n",
       " 'Exultant': 839,\n",
       " 'Eyebrows': 840,\n",
       " 'Eyeing': 841,\n",
       " 'Eyes': 842,\n",
       " 'FIRE': 843,\n",
       " 'Face': 844,\n",
       " 'Faced': 845,\n",
       " 'Faces': 846,\n",
       " 'Facquerware': 847,\n",
       " 'Failing': 848,\n",
       " 'Faintly': 849,\n",
       " 'Fair': 850,\n",
       " 'Falcon': 851,\n",
       " 'Falling': 852,\n",
       " 'Fangs': 853,\n",
       " 'Fanned': 854,\n",
       " 'Far': 855,\n",
       " 'Farewell': 856,\n",
       " 'Farmer': 857,\n",
       " 'Farmers': 858,\n",
       " 'Farther': 859,\n",
       " 'Fast': 860,\n",
       " 'Father': 861,\n",
       " 'Fearful': 862,\n",
       " 'Fearfully': 863,\n",
       " 'Fearing': 864,\n",
       " 'Feast': 865,\n",
       " 'Feebly': 866,\n",
       " 'Feel': 867,\n",
       " 'Feeling': 868,\n",
       " 'Feels': 869,\n",
       " 'Feet': 870,\n",
       " 'Feigning': 871,\n",
       " 'Female': 872,\n",
       " 'Ferry': 873,\n",
       " 'Festival': 874,\n",
       " 'Fetch': 875,\n",
       " 'Few': 876,\n",
       " 'Fief': 877,\n",
       " 'Field': 878,\n",
       " 'Fiends': 879,\n",
       " 'Fight': 880,\n",
       " 'Fighting': 881,\n",
       " 'Fights': 882,\n",
       " 'Filial': 883,\n",
       " 'Fill': 884,\n",
       " 'Filled': 885,\n",
       " 'Filling': 886,\n",
       " 'Filthy': 887,\n",
       " 'Finally': 888,\n",
       " 'Finance': 889,\n",
       " 'Financial': 890,\n",
       " 'Find': 891,\n",
       " 'Finding': 892,\n",
       " 'Fine': 893,\n",
       " 'Finishing': 894,\n",
       " 'Fire': 895,\n",
       " 'Fireworks': 896,\n",
       " 'Firmly': 897,\n",
       " 'First': 898,\n",
       " 'Fishermen': 899,\n",
       " 'Five': 900,\n",
       " 'Fix': 901,\n",
       " 'Flames': 902,\n",
       " 'Flashing': 903,\n",
       " 'Fleeing': 904,\n",
       " 'Flexing': 905,\n",
       " 'Flicking': 906,\n",
       " 'Flies': 907,\n",
       " 'Flinging': 908,\n",
       " 'Floating': 909,\n",
       " 'Flower': 910,\n",
       " 'Flowers': 911,\n",
       " 'Flushing': 912,\n",
       " 'Flutes': 913,\n",
       " 'Flying': 914,\n",
       " 'Follow': 915,\n",
       " 'Following': 916,\n",
       " 'Food': 917,\n",
       " 'Fooks': 918,\n",
       " 'Fool': 919,\n",
       " 'Foolishness': 920,\n",
       " 'Fools': 921,\n",
       " 'Footsteps': 922,\n",
       " 'For': 923,\n",
       " 'Force': 924,\n",
       " 'Forced': 925,\n",
       " 'Ford': 926,\n",
       " 'Forest': 927,\n",
       " 'Foreword': 928,\n",
       " 'Forget': 929,\n",
       " 'Forgetfulness': 930,\n",
       " 'Forgetting': 931,\n",
       " 'Forgive': 932,\n",
       " 'Fortunately': 933,\n",
       " 'Fortune': 934,\n",
       " 'Forty': 935,\n",
       " 'Forty-seven': 936,\n",
       " 'Foundation': 937,\n",
       " 'Four': 938,\n",
       " 'Fourth': 939,\n",
       " 'Fove': 940,\n",
       " 'Fox': 941,\n",
       " 'Foxes': 942,\n",
       " 'Freezing': 943,\n",
       " 'Fresh': 944,\n",
       " 'Freshly': 945,\n",
       " 'Friend': 946,\n",
       " 'Friends': 947,\n",
       " 'Frightened': 948,\n",
       " 'Frightening': 949,\n",
       " 'Frighteningly': 950,\n",
       " 'Frigid': 951,\n",
       " 'From': 952,\n",
       " 'Frowning': 953,\n",
       " 'Frustrated': 954,\n",
       " 'Fuchikawa': 955,\n",
       " 'Fuchu': 956,\n",
       " 'Fudesute': 957,\n",
       " 'Fugen': 958,\n",
       " 'Fuji': 959,\n",
       " 'Fujisawa': 960,\n",
       " 'Fujiwara': 961,\n",
       " 'Fukiage': 962,\n",
       " 'Fukien': 963,\n",
       " 'Fukushima': 964,\n",
       " 'Fully': 965,\n",
       " 'Fumbling': 966,\n",
       " 'Fumiko': 967,\n",
       " 'Funabashi': 968,\n",
       " 'Funabashi-sama—Mr': 969,\n",
       " 'Funashima': 970,\n",
       " 'Funashima—the': 971,\n",
       " 'Funny': 972,\n",
       " 'Funny-looking': 973,\n",
       " 'Furthermore': 974,\n",
       " 'Fuses': 975,\n",
       " 'Fushikian': 976,\n",
       " 'Fushimi': 977,\n",
       " 'Fushimi—without': 978,\n",
       " 'Futamigaura': 979,\n",
       " 'Fute': 980,\n",
       " 'Fuwa': 981,\n",
       " 'Gagging': 982,\n",
       " 'Gamo': 983,\n",
       " 'Ganrin': 984,\n",
       " 'Ganryu': 985,\n",
       " 'Ganryu—': 986,\n",
       " 'Gasping': 987,\n",
       " 'Gasps': 988,\n",
       " 'Gate': 989,\n",
       " 'Gateway': 990,\n",
       " 'Gaze': 991,\n",
       " 'Gazing': 992,\n",
       " 'Geese': 993,\n",
       " 'Geki': 994,\n",
       " 'Gempachi': 995,\n",
       " 'Gen': 996,\n",
       " 'General': 997,\n",
       " 'Genji': 998,\n",
       " 'Genjiro': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a simple text tokenizer that handles unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed] #If word not found give it junk value\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self,ids):\n",
    "        if isinstance(ids, int):  # Modified decoder if a single integer is passed, convert it to a list\n",
    "            ids = [ids]\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'Hello, do you like tea?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1155, 6, 7511, 19982, 11618, 17827, 94]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer(vocab)\n",
    "print(tokenizer.encode(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea?\n"
     ]
    }
   ],
   "source": [
    "text1_encoded = tokenizer.encode(text1)\n",
    "\n",
    "print(tokenizer.decode(text1_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Sampling with Sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_text = tokenizer.encode(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove first 1500 tokens from sample \n",
    "enc_sample = enc_text[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [1739, 437, 776, 3495]\n",
      "y:      [437, 776, 3495, 3143]\n"
     ]
    }
   ],
   "source": [
    "#create input-target pairs for the next word prediction\n",
    "\n",
    "context_size = 4\n",
    "x = enc_text[:context_size]\n",
    "y = enc_text[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10644] ----> 17968\n",
      "[10644, 17968] ----> 19578\n",
      "[10644, 17968, 19578] ----> 11089\n",
      "[10644, 17968, 19578, 11089] ----> 12982\n"
     ]
    }
   ],
   "source": [
    "#create next word prediction task\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in ----> the\n",
      "in the ----> western\n",
      "in the western ----> island\n",
      "in the western island ----> of\n"
     ]
    }
   ],
   "source": [
    "#Use text instead of token Ids\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode(desired))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of **Dataset and DataLoader classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader helps GPT with preprocessing of data by splitting data into **Tokenized text**, **Overlapped Sequences**, **Creating inputs and next token targets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(text)\n",
    "\n",
    "        #Using sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i: i + max_length]\n",
    "            target_chunk = token_ids[i+1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    \n",
    "    #Returns total single row number of dataset\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    #Returns a single row from dataset\n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle= True, drop_last= True, num_workers=0):\n",
    "    tokenizer = SimpleTokenizer(vocab) #initalizes tokenizer\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) #Creates Dataset\n",
    "    dataloader = DataLoader(dataset, batch_size= batch_size, shuffle=shuffle, drop_last = drop_last, num_workers= num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1739,  437,  776, 3495]]), tensor([[ 437,  776, 3495, 3143]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_data_loader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle= False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `first_batch` variable contains two tensors: The first tensor stores the input token IDs and the second tensor stores the target token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 437,  776, 3495, 3143]]), tensor([[ 776, 3495, 3143, 9209]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20044\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0877, -0.6113,  0.3441,  ...,  0.7630,  0.2497, -0.5980],\n",
      "        [-0.5935, -0.0849,  0.2489,  ..., -1.0397,  0.1785, -0.3177],\n",
      "        [ 0.1765,  0.7711,  0.4033,  ..., -0.1258,  0.3940,  0.9446],\n",
      "        ...,\n",
      "        [ 0.7967,  0.4206, -0.3368,  ...,  1.2137,  2.2570,  0.0112],\n",
      "        [ 0.1894, -0.5221, -1.0441,  ...,  1.0416, -0.1292,  0.3845],\n",
      "        [-1.3862, -0.2585, -0.3558,  ..., -0.4617, -1.7202,  0.1962]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(25)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1739,  437,  776, 3495])\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(x)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.9027, -0.2900,  1.3043,  ..., -0.7227,  0.2779,  1.2826],\n",
      "        [ 1.7247, -0.0518,  1.1980,  ...,  1.3852, -0.9934,  1.0998],\n",
      "        [-1.7076,  1.0034, -0.3534,  ...,  0.7094, -1.5231, -0.3077],\n",
      "        [-0.6164, -0.4027, -1.7662,  ..., -0.0221, -1.9148,  2.6982]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  437,   776,  3495,  3143],\n",
      "        [ 9209, 17968,  1408,  5356],\n",
      "        [  478,  2468,    11,  3028],\n",
      "        [  928,  5356,   772,  2051],\n",
      "        [   11,  2389,  1565,  1354],\n",
      "        [ 3106, 20043,  1991,  3491],\n",
      "        [20043,  1704,  2256, 13042],\n",
      "        [13300,    85,     6,  9209]])\n",
      "torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4 \n",
    "dataloader = create_data_loader_v1(raw_text, batch_size=8, max_length= max_length, stride= max_length, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(targets)\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that each token ID is now embeded as a 256-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two types of positional embeddings: absolute and relative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.3595, -0.4262,  0.3191,  ...,  1.3478, -0.6671,  0.7288],\n",
       "         [ 0.5450, -1.2454,  1.4665,  ...,  1.7144, -0.7670,  1.3902],\n",
       "         [-0.8712, -0.2022, -0.4211,  ...,  0.0714, -2.1658, -0.2321],\n",
       "         [ 0.3051,  1.4921, -1.4002,  ..., -0.9770, -2.2932,  3.3520]],\n",
       "\n",
       "        [[ 2.6149, -1.4596, -1.2166,  ...,  1.0100,  0.1937,  0.2448],\n",
       "         [-0.9907, -1.2723, -2.3366,  ...,  1.5804,  2.6588,  0.7619],\n",
       "         [ 1.3230, -0.8819,  0.9740,  ..., -1.0369, -2.2120,  0.8156],\n",
       "         [ 1.0983,  2.3996,  1.3454,  ...,  1.6240, -0.4838,  0.6245]],\n",
       "\n",
       "        [[ 0.7572, -0.9177, -1.7751,  ...,  1.7690, -0.7205, -0.0769],\n",
       "         [-0.3805, -1.4515,  2.7671,  ...,  2.0395,  0.2330,  0.1416],\n",
       "         [ 2.4902, -2.1226, -0.1881,  ..., -1.4347, -0.3249, -0.0801],\n",
       "         [ 1.1630,  1.9819,  2.4968,  ..., -2.4278,  0.0127, -1.0046]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.6261,  0.3196,  0.8544,  ...,  3.8293,  0.4749,  0.4261],\n",
       "         [-0.0888, -0.4508,  0.4112,  ...,  0.8742, -1.4179,  1.8009],\n",
       "         [-0.5499, -1.4641, -0.4236,  ..., -1.0997, -2.3629,  0.2717],\n",
       "         [ 1.4581,  2.0590, -0.1779,  ..., -0.3930, -0.8744,  0.8536]],\n",
       "\n",
       "        [[ 1.4117, -1.2686,  0.2871,  ...,  2.7223, -1.0201, -0.1678],\n",
       "         [-2.5660, -1.4521, -0.0873,  ..., -0.1325, -1.4939,  0.4866],\n",
       "         [ 0.9775, -0.3965, -0.8586,  ...,  1.0476, -1.9948, -0.4332],\n",
       "         [ 2.4222,  3.8853, -2.0337,  ..., -1.7400,  0.1037,  1.6729]],\n",
       "\n",
       "        [[ 1.2457, -0.6942,  2.0008,  ...,  1.6529, -1.3792, -0.0925],\n",
       "         [-0.8883,  0.1538, -2.1316,  ..., -1.5419,  0.0205,  0.6389],\n",
       "         [ 1.1707, -0.1397, -2.4704,  ..., -1.8078, -0.7529, -0.1328],\n",
       "         [ 0.1358,  1.6147, -0.8508,  ..., -1.4759, -1.7372,  0.5657]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a LLM from Scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in story as sample txt into python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 2649881\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MUSASHI \n",
      "\n",
      "\n",
      "By Eiji Yoshikawa \n",
      "\n",
      "Translated from the Japanese by Charles S. Terry \n",
      "Foreword by Edwin O. Reischauer \n",
      "\n",
      "\n",
      "\n",
      "Kodansha Internatio\n"
     ]
    }
   ],
   "source": [
    "with open(\"/Users/Scott/Library/Mobile Documents/com~apple~CloudDocs/CVs and LOMs/Portfolio/DS 2024/GPT architeture/Musashi.txt\", 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[0:149])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of tokens in the text are: 596013\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print('The amount of tokens in the text are:' , len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Tokens into Token Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20044\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token: integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '\"': 1,\n",
       " \"'\": 2,\n",
       " '(': 3,\n",
       " ')': 4,\n",
       " '*': 5,\n",
       " ',': 6,\n",
       " '-eight': 7,\n",
       " '-five': 8,\n",
       " '-seven': 9,\n",
       " '-six': 10,\n",
       " '.': 11,\n",
       " '/': 12,\n",
       " '/His': 13,\n",
       " '/Run': 14,\n",
       " '/clopping': 15,\n",
       " '/more': 16,\n",
       " '0': 17,\n",
       " '0-over': 18,\n",
       " '000': 19}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "#Slice vocab words to show first 100 vocab tokens \n",
    "dict(itertools.islice(vocab.items(),20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a simple text tokenizer that handles unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed] #If word not found give it junk value\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self,ids):\n",
    "        if isinstance(ids, int):  # Modified decoder if a single integer is passed, convert it to a list\n",
    "            ids = [ids]\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'Hello, do you like tea?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1155, 6, 7511, 19982, 11618, 17827, 94]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer(vocab)\n",
    "print(tokenizer.encode(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea?\n"
     ]
    }
   ],
   "source": [
    "text1_encoded = tokenizer.encode(text1)\n",
    "\n",
    "print(tokenizer.decode(text1_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Sampling with Sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_text = tokenizer.encode(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove first 1500 tokens from sample \n",
    "enc_sample = enc_text[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [1739, 437, 776, 3495]\n",
      "y:      [437, 776, 3495, 3143]\n"
     ]
    }
   ],
   "source": [
    "#create input-target pairs for the next word prediction\n",
    "\n",
    "context_size = 4\n",
    "x = enc_text[:context_size]\n",
    "y = enc_text[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10644] ----> 17968\n",
      "[10644, 17968] ----> 19578\n",
      "[10644, 17968, 19578] ----> 11089\n",
      "[10644, 17968, 19578, 11089] ----> 12982\n"
     ]
    }
   ],
   "source": [
    "#create next word prediction task\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in ----> the\n",
      "in the ----> western\n",
      "in the western ----> island\n",
      "in the western island ----> of\n"
     ]
    }
   ],
   "source": [
    "#Use text instead of token Ids\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode(desired))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of **Dataset and DataLoader classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader helps GPT with preprocessing of data by splitting data into **Tokenized text**, **Overlapped Sequences**, **Creating inputs and next token targets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(text)\n",
    "\n",
    "        #Using sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i: i + max_length]\n",
    "            target_chunk = token_ids[i+1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    \n",
    "    #Returns total single row number of dataset\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    #Returns a single row from dataset\n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle= True, drop_last= True, num_workers=0):\n",
    "    tokenizer = SimpleTokenizer(vocab) #initalizes tokenizer\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) #Creates Dataset\n",
    "    dataloader = DataLoader(dataset, batch_size= batch_size, shuffle=shuffle, drop_last = drop_last, num_workers= num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1739,  437,  776, 3495]]), tensor([[ 437,  776, 3495, 3143]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_data_loader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle= False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `first_batch` variable contains two tensors: The first tensor stores the input token IDs and the second tensor stores the target token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 437,  776, 3495, 3143]]), tensor([[ 776, 3495, 3143, 9209]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20044\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0877, -0.6113,  0.3441,  ...,  0.7630,  0.2497, -0.5980],\n",
      "        [-0.5935, -0.0849,  0.2489,  ..., -1.0397,  0.1785, -0.3177],\n",
      "        [ 0.1765,  0.7711,  0.4033,  ..., -0.1258,  0.3940,  0.9446],\n",
      "        ...,\n",
      "        [ 0.7967,  0.4206, -0.3368,  ...,  1.2137,  2.2570,  0.0112],\n",
      "        [ 0.1894, -0.5221, -1.0441,  ...,  1.0416, -0.1292,  0.3845],\n",
      "        [-1.3862, -0.2585, -0.3558,  ..., -0.4617, -1.7202,  0.1962]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(25)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1739,  437,  776, 3495])\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(x)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.9027, -0.2900,  1.3043,  ..., -0.7227,  0.2779,  1.2826],\n",
      "        [ 1.7247, -0.0518,  1.1980,  ...,  1.3852, -0.9934,  1.0998],\n",
      "        [-1.7076,  1.0034, -0.3534,  ...,  0.7094, -1.5231, -0.3077],\n",
      "        [-0.6164, -0.4027, -1.7662,  ..., -0.0221, -1.9148,  2.6982]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  437,   776,  3495,  3143],\n",
      "        [ 9209, 17968,  1408,  5356],\n",
      "        [  478,  2468,    11,  3028],\n",
      "        [  928,  5356,   772,  2051],\n",
      "        [   11,  2389,  1565,  1354],\n",
      "        [ 3106, 20043,  1991,  3491],\n",
      "        [20043,  1704,  2256, 13042],\n",
      "        [13300,    85,     6,  9209]])\n",
      "torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4 \n",
    "dataloader = create_data_loader_v1(raw_text, batch_size=8, max_length= max_length, stride= max_length, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(targets)\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that each token ID is now embeded as a 256-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two types of positional embeddings: absolute and relative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.3595, -0.4262,  0.3191,  ...,  1.3478, -0.6671,  0.7288],\n",
       "         [ 0.5450, -1.2454,  1.4665,  ...,  1.7144, -0.7670,  1.3902],\n",
       "         [-0.8712, -0.2022, -0.4211,  ...,  0.0714, -2.1658, -0.2321],\n",
       "         [ 0.3051,  1.4921, -1.4002,  ..., -0.9770, -2.2932,  3.3520]],\n",
       "\n",
       "        [[ 2.6149, -1.4596, -1.2166,  ...,  1.0100,  0.1937,  0.2448],\n",
       "         [-0.9907, -1.2723, -2.3366,  ...,  1.5804,  2.6588,  0.7619],\n",
       "         [ 1.3230, -0.8819,  0.9740,  ..., -1.0369, -2.2120,  0.8156],\n",
       "         [ 1.0983,  2.3996,  1.3454,  ...,  1.6240, -0.4838,  0.6245]],\n",
       "\n",
       "        [[ 0.7572, -0.9177, -1.7751,  ...,  1.7690, -0.7205, -0.0769],\n",
       "         [-0.3805, -1.4515,  2.7671,  ...,  2.0395,  0.2330,  0.1416],\n",
       "         [ 2.4902, -2.1226, -0.1881,  ..., -1.4347, -0.3249, -0.0801],\n",
       "         [ 1.1630,  1.9819,  2.4968,  ..., -2.4278,  0.0127, -1.0046]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.6261,  0.3196,  0.8544,  ...,  3.8293,  0.4749,  0.4261],\n",
       "         [-0.0888, -0.4508,  0.4112,  ...,  0.8742, -1.4179,  1.8009],\n",
       "         [-0.5499, -1.4641, -0.4236,  ..., -1.0997, -2.3629,  0.2717],\n",
       "         [ 1.4581,  2.0590, -0.1779,  ..., -0.3930, -0.8744,  0.8536]],\n",
       "\n",
       "        [[ 1.4117, -1.2686,  0.2871,  ...,  2.7223, -1.0201, -0.1678],\n",
       "         [-2.5660, -1.4521, -0.0873,  ..., -0.1325, -1.4939,  0.4866],\n",
       "         [ 0.9775, -0.3965, -0.8586,  ...,  1.0476, -1.9948, -0.4332],\n",
       "         [ 2.4222,  3.8853, -2.0337,  ..., -1.7400,  0.1037,  1.6729]],\n",
       "\n",
       "        [[ 1.2457, -0.6942,  2.0008,  ...,  1.6529, -1.3792, -0.0925],\n",
       "         [-0.8883,  0.1538, -2.1316,  ..., -1.5419,  0.0205,  0.6389],\n",
       "         [ 1.1707, -0.1397, -2.4704,  ..., -1.8078, -0.7529, -0.1328],\n",
       "         [ 0.1358,  1.6147, -0.8508,  ..., -1.4759, -1.7372,  0.5657]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

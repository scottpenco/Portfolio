{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG on OPEN AI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import GrobidParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from Grobid\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    \"/Users/Scott/Downloads/test input/\",\n",
    "    glob=\"*\",\n",
    "    suffixes=[\".pdf\"],\n",
    "    parser=GrobidParser(segment_sentences=False),\n",
    ")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'In this paper, we introduce Dynamically Rewired Message Passing (DRew), a novel framework for layer-dependent, multi-hop message passing that takes a principled approach to information flow, is robust to over-squashing, and can be applied to any MPNN for deep learning on graphs.',\n",
       " 'para': '0',\n",
       " 'bboxes': \"[[{'page': '1', 'x': '307.44', 'y': '303.89', 'h': '234.00', 'w': '9.03'}, {'page': '1', 'x': '307.44', 'y': '315.85', 'h': '235.25', 'w': '9.03'}, {'page': '1', 'x': '307.44', 'y': '328.19', 'h': '234.00', 'w': '8.64'}, {'page': '1', 'x': '307.44', 'y': '340.15', 'h': '234.00', 'w': '8.64'}, {'page': '1', 'x': '307.44', 'y': '352.10', 'h': '202.11', 'w': '8.64'}]]\",\n",
       " 'pages': \"('1', '1')\",\n",
       " 'section_title': 'Introduction',\n",
       " 'section_number': '1.',\n",
       " 'paper_title': 'DRew: Dynamically Rewired Message Passing with Delay',\n",
       " 'file_path': '/Users/Scott/Downloads/test input/2305.08018v2.pdf'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking metadata\n",
    "data[1].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the documents and vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
    "import os\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key =  #removed for privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/grobid_env/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "#Split the document using RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap = 100) # type: ignore\n",
    "docs = splitter.split_documents(data) \n",
    "\n",
    "#Embed the documents in a persistent Chroma vector Database\n",
    "embedding_function = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "vectorstore = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=os.getcwd()\n",
    ")\n",
    "\n",
    "\n",
    "#Configure the vectore sotre as a retriever \n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":3}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a retrieval prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add placeholders to the message string\n",
    "message = \"\"\"\n",
    "Answer the following question using the context provided:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create a chat prompt template from the message string\n",
    "prompt_template = ChatPromptTemplate.from_messages([(\"human\", message)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46400 hours of computation were performed on the Tesla V100-SXM2-32GB hardware.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "openai_api_key = openai_api_key\n",
    "\n",
    "#Store Documents to be available to retrieval\n",
    "vectorstore = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding=OpenAIEmbeddings(openai_api_key=openai_api_key),\n",
    "    persist_directory=os.getcwd()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Chain to link retriever, prompt_template, and llm\n",
    "rag_chain = ({\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt_template\n",
    "            | llm)\n",
    "\n",
    "# Invoking chain and printing response\n",
    "response = rag_chain.invoke(\"How many hours of computation were performed on the Tesla V100-SXM2-32GB hardware?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Question:** What was the carbon efficiency of the private infrastructure used in the experiments?\n",
      "**Answer:** The carbon efficiency of the private infrastructure was 0.432 kgCO2 eq/kWh.\n",
      "\n",
      "2. **Question:** How many hours of computation were performed on the Tesla V100-SXM2-32GB hardware?\n",
      "**Answer:** A cumulative of 46400 hours of computation was performed on the Tesla V100-SXM2-32GB hardware.\n",
      "\n",
      "3. **Question:** What was the total estimated emissions in kgCO2 eq for the experiments conducted?\n",
      "**Answer:** The total estimated emissions for the experiments were 6013.44 kgCO2 eq.\n",
      "\n",
      "4. **Question:** Why is there overlap between the pre-training and fine-tuning small molecule datasets?\n",
      "**Answer:** There is overlap between the datasets because under a certain number of heavy atoms, nearly all possible molecules can be enumerated, and many datasets draw from this distribution.\n",
      "\n",
      "5. **Question:** What are the differences between the molecules in ANI-1x and QM9 datasets?\n",
      "**Answer:** While there is overlap between the molecules in ANI-1x and QM9 datasets, the level of DFT used to generate the data is different, the labels are not identical, and the 3D structures may vary.\n",
      "\n",
      "6. **Question:** What does Extended Data Figure 10 show?\n",
      "**Answer:** Extended Data Figure 10 displays the ligand enrichment score, receptor enrichment score, and enrichment in each condition for all LR pair modules.\n",
      "\n",
      "7. **Question:** How is the ligand enrichment score represented in Extended Data Figure 10?\n",
      "**Answer:** The ligand enrichment score is represented in Extended Data Figure 10 for all LR pair modules.\n",
      "\n",
      "8. **Question:** What information is provided in the Extended Data Figure 10 for the LR pair modules?\n",
      "**Answer:** Extended Data Figure 10 provides information on the ligand enrichment score, receptor enrichment score, and enrichment in each condition for all LR pair modules.\n",
      "\n",
      "9. **Question:** How do the thermodynamic properties in ANI-1x and QM9 datasets relate to the total electronic energy?\n",
      "**Answer:** The thermodynamic properties in ANI-1x and QM9 datasets are closely related to the total electronic energy, although the labels are not identical.\n",
      "\n",
      "10. **Question:** Why is there always some overlap between small molecule datasets?\n",
      "**Answer:** There is always some overlap between small molecule datasets because nearly all possible molecules can be enumerated under a certain number of heavy atoms, leading to dataset distributions that draw from similar sets of molecules.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks specifically about the provided PDF documents.\"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. Do not use any external knowledge \"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"Create 10 scientific question and answer pairs  related to the PDFs\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grobid_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
